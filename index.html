<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
    body {
        font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight: 300;
        font-size: 17px;
        margin-left: auto;
        margin-right: auto;
        width: 980px;
    }

    h1 {
        font-weight: 300;
        line-height: 1.15em;
    }

    h2 {
        font-size: 1.75em;
    }

    a:link, a:visited {
        color: #B6486F;
        text-decoration: none;
    }

    a:hover {
        color: #208799;
    }

    h1, h2, h3 {
        text-align: center;
    }

    h1 {
        font-size: 40px;
        font-weight: 500;
    }

    h2 {
        font-weight: 400;
        margin: 16px 0px 4px 0px;
    }

    .paper-title {
        padding: 16px 0px 16px 0px;
    }

    section {
        margin: 32px 0px 32px 0px;
        text-align: justify;
        clear: both;
    }

    .col-5 {
        width: 20%;
        float: left;
    }

    .col-4 {
        width: 25%;
        float: left;
    }

    .col-3 {
        width: 33%;
        float: left;
    }

    .col-2 {
        width: 50%;
        float: left;
    }

    .col-1 {
        width: 100%;
        float: left;
    }

    .row, .author-row, .affil-row {
        overflow: auto;
    }

    .author-row, .affil-row {
        font-size: 26px;
    }

    .row {
        margin: 16px 0px 16px 0px;
    }

    .authors {
        font-size: 26px;
    }

    .affil-row {
        margin-top: 16px;
    }

    .teaser {
        max-width: 100%;
    }

    .text-center {
        text-align: center;
    }

    .screenshot {
        width: 256px;
        border: 1px solid #ddd;
    }

    .screenshot-el {
        margin-bottom: 16px;
    }

    hr {
        height: 1px;
        border: 0;
        border-top: 1px solid #ddd;
        margin: 0;
    }

    .material-icons {
        vertical-align: -6px;
    }

    p {
        line-height: 1.25em;
    }

    .caption {
        font-size: 16px;
        /*font-style: italic;*/
        color: #666;
        text-align: center;
        margin-top: 4px;
        margin-bottom: 10px;
    }

    video {
        display: block;
        margin: auto;
    }

    figure {
        display: block;
        margin: auto;
        margin-top: 10px;
        margin-bottom: 10px;
    }

    #bibtex pre {
        font-size: 13.5px;
        background-color: #eee;
        padding: 16px;
    }

    .blue {
        color: #2c82c9;
        font-weight: bold;
    }

    .orange {
        color: #d35400;
        font-weight: bold;
    }

    .flex-row {
        display: flex;
        flex-flow: row wrap;
        justify-content: space-around;
        padding: 0;
        margin: 0;
        list-style: none;
    }

    .paper-btn {
        position: relative;
        text-align: center;

        display: inline-block;
        margin: 8px;
        padding: 8px 8px;

        border-width: 0;
        outline: none;
        border-radius: 2px;

        background-color: #B6486F;
        color: white !important;
        font-size: 20px;
        width: 100px;
        font-weight: 600;
    }

    .paper-btn-parent {
        display: flex;
        justify-content: center;
        margin: 16px 0px;
    }

    .paper-btn:hover {
        opacity: 0.85;
    }

    .container {
        margin-left: auto;
        margin-right: auto;
        padding-left: 16px;
        padding-right: 16px;
    }

    .venue {
        /*color: #B6486F;*/
        font-size: 30px;

    }

</style>

<!-- End : Google Analytics Code-->
<script type="text/javascript" src="../js/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic'
      rel='stylesheet' type='text/css'>

<body>
<div class="container">
    <div class="paper-title">
        <h1>Compositional Text-to-Image Generation with Dense Blob Representations</h1>
    </div>

    <div id="authors">
        <div class="author-row">
            <div class="text-center"><font color="green">ICML 2024 Submission 5042 Anonymous Authors</font></div>
        </div>
    </div>

    <section id="results0">
        <h2>Part I: Visualizations of how blob representations capture irregular objects, large objects and
            background</h2>
        <hr>
        <div class="flex-row">
            <p>
                Here we show some visual examples of how blob-grounded generation (BlobGEN) can capture irregular
                objects, large objects and background. For example, the first three examples in (a) show that blob
                representations can capture "a person with waving hands outside the blob". the last two examples in (a)
                show that blob representations can capture "a river with irregular shapes". Also, the fourth example in
                (b)
                also shows that blob representations can capture "the great wall with a zigzag shape". As for large
                objects and background, the first two rows in (b) show that blob representations can capture "sky" with
                the similar color and mood. The second row in (b) show that blob representations can capture the large
                "pier" with a similar color, pattern and shape. The third row in (b) show that blob representations
                can capture the large
                "foggy grass" and its reflection in the water. The last two rows in (b) show that blob representations
                can capture the large mountains in the background.
            </p>
        </div>
        <figure style="width: 100%;">
            <a href="assets/blobs_irregular.png">
                <center><img width="100%" src="assets/blobs_irregular.png"></center>
            </a>
            <p class="caption" style="margin-bottom: 24px;">
                Examples of blob-grounded generation capturing (a) irregular objects (e.g. "person with waving
                hands" and "river"), and (b) large objects and background (e.g., "sky", "mountain" and "grass").
                From left to right in each row, we show the reference real image, blobs, generated image and
                generated image superposed on blobs. Note that images have been compressed to save memory. Better
                zoom in for better visualization.
            </p>
        </figure>
    </section>

    <section id="results1"/>
    <h2>Part II: Qualitative results of comparing our method with LMD [1]</h2>
    <hr>
    <div class="flex-row">
        <p>
            Here we compare the qualitative results of our method and LMD [1]. For a fair comparison, we use <b>the
            same 8 fixed in-context demo examples</b> (without retrieval) for GPT-4 to generate bounding boxes for LMD
            and blobs for our
            method, respectively. We observe that 1) for some complex examples, such as "a boat to the right of a fork"
            in (a) and "there are one car with three motorcycles in the image" in (b), LMD fails but our method works;
            2)
            LMD consistently has the "copy-and-paste" artifact in its generated images (since it modifies the diffusion
            sampling process for compositionality), such as "a teddy bear to the left of a potted plant" in
            (a) and "a photo of four boats" in (b), while our generation looks much more natural; and 3) the
            sampling time of LMD is around 3x slower than our method.

        </p>
    </div>
    <figure style="width: 100%;">
        <a href="assets/blobs_comp_lmd.png">
            <center><img width="100%" src="assets/blobs_comp_lmd.png"></center>
        </a>
        <p class="caption" style="margin-bottom: 24px;">
            Qualitative results of comparing our method with LMD [1] on the NSR-1k benchmark proposed by LayoutGPT [2]
            for spatial and numerical reasoning. In each example, we first prompt GPT-4 to generate boxes for LMD and
            blobs for our method, respectively, with <b>the same 8 fixed in-context demo examples</b>. The generated
            boxes and blobs are passed to LMD and BlobGEN to generate images, respectively. Note that images have been
            compressed to save memory. Better zoom in for better visualization.
        </p>
    </figure>
    </section>


    <section id="results2">
        <h2>Part III: Qualitative results of blob control over using LLMs</h2>
        <hr>
        <div class="flex-row">
            <p>
                To further demonstrate the blob control from LLMs, we consider four cases of how LLMs understand
                compositional prompts for correct visual planning: (a) swapping object name ("cat" <-> "car"), (b)
                changing relative reposition ("left" <-> "right"), (c) changing object number ("three" <-> "four"), and
                (d) swapping object number ("one bench & two cats" <-> "two benches & one cat"). We can see that LLMs
                has the ability of capturing the subtle differences when the prompts are modified in an "adversarial"
                manner. Besides, LLMs can generate diverse and feasible visual layouts from the same prompt, which
                BlobGEN can use for robustly synthesizing correct images of high quality and diversity.
            </p>
        </div>
        <figure style="width: 100%;">
            <a href="assets/blobs_llm_control.png">
                <center><img width="100%" src="assets/blobs_llm_control.png"></center>
            </a>
            <p class="caption" style="margin-bottom: 24px;">
                Qualitative results of blob control over using LLMs, where we consider four cases of how LLMs understand
                compositional prompts for correct visual planning: (a) swapping object name ("cat" <-> "car"), (b)
                changing relative reposition ("left" <-> "right"), (c) changing object number ("three" <-> "four"), and
                (d) swapping object number ("one bench & two cats" <-> "two benches & one cat"). In each example, we
                show diverse blobs generated by LLMs (bottom) and the corresponding images generated by BlobGEN (top)
                from the
                same text prompt. Note that images have been compressed to save memory. Better zoom in for better
                visualization.
            </p>
        </figure>
    </section>

    <section id="results3">
        <h2>Part IV: More comparison results of blob- and box-grounded generation</h2>
        <hr>
        <div class="flex-row">
            <p>
                To further demonstrate that blob representations can capture more fine-grained information than bounding
                boxes, we provide more qualitative results of comparing blob-grounded generation (Ours) and box-grounded
                generation (GLIGEN [3]). We can clearly see that generated images from blob representations can better
                reconstruct fine-grained details of the reference image than those from bounding boxes. For instance, in
                the first example, the
                laptop's orientation and the two books' orientation and color in the generated image from our method
                closely follow the
                reference image while GLIGEN does not. In the second example, the car's color and orientation and the
                grass and road's appearance and orientation from our method also more closely follow the
                reference image. In the third example, the two giraffes' orientation is better captured by blob
                representations than bounding boxes, with a direct comparison to the reference. In the last example, the
                two girls' appearance and pose, the couch's color, and the orientation and appearance of the
                counter and chair in the background are also better captured by blob representations.
            </p>
        </div>
        <figure style="width: 100%;">
            <a href="assets/blobs_comp_gligen.png">
                <center><img width="100%" src="assets/blobs_comp_gligen.png"></center>
            </a>
            <p class="caption" style="margin-bottom: 24px;">
                More qualitative results of comparing blob-grounded generation (Ours) and box-grounded generation
                (GLIGEN [3]). In each row, we visualize the reference real image (Left), bounding boxes and GLIGEN
                generated image (Middle), blobs and our generated image (Right). We also highlight the salient objects
                in each example to guide the scrutinization of how the respective generated image differs from reference
                image. Note that images have been compressed to save memory. Better zoom in for better
                visualization.
            </p>
        </figure>
    </section>


    <section id="advantages"/>
    <h2>References</h2>
    <hr>
    <div class="flex-row">
        <p>
            [1] L. Lian, et al. "LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion
            Models with Large Language Models.", TMLR 2023.
            <br>
            [2] W. Feng, et al. "LayoutGPT: compositional visual planning and generation with large language models.",
            NeurIPS 2023.
            <br>
            [3] Y. Li, et al. "GLIGEN: Open-set grounded text-to-image generation.", CVPR 2023.
        </p>
    </div>
    </section>


</div>
</body>
</html>
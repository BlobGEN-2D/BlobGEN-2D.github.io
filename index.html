<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
    body {
        font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight: 300;
        font-size: 17px;
        margin-left: auto;
        margin-right: auto;
        width: 980px;
    }

    h1 {
        font-weight: 300;
        line-height: 1.15em;
    }

    h2 {
        font-size: 1.75em;
    }

    a:link, a:visited {
        color: #B6486F;
        text-decoration: none;
    }

    a:hover {
        color: #208799;
    }

    h1, h2, h3 {
        text-align: center;
    }

    h1 {
        font-size: 40px;
        font-weight: 500;
    }

    h2 {
        font-weight: 400;
        margin: 16px 0px 4px 0px;
    }

    .paper-title {
        padding: 16px 0px 16px 0px;
    }

    section {
        margin: 32px 0px 32px 0px;
        text-align: justify;
        clear: both;
    }

    .col-5 {
        width: 20%;
        float: left;
    }

    .col-4 {
        width: 25%;
        float: left;
    }

    .col-3 {
        width: 33%;
        float: left;
    }

    .col-2 {
        width: 50%;
        float: left;
    }

    .col-1 {
        width: 100%;
        float: left;
    }

    .row, .author-row, .affil-row {
        overflow: auto;
    }

    .author-row, .affil-row {
        font-size: 26px;
    }

    .row {
        margin: 16px 0px 16px 0px;
    }

    .authors {
        font-size: 26px;
    }

    .affil-row {
        margin-top: 16px;
    }

    .teaser {
        max-width: 100%;
    }

    .text-center {
        text-align: center;
    }

    .screenshot {
        width: 256px;
        border: 1px solid #ddd;
    }

    .screenshot-el {
        margin-bottom: 16px;
    }

    hr {
        height: 1px;
        border: 0;
        border-top: 1px solid #ddd;
        margin: 0;
    }

    .material-icons {
        vertical-align: -6px;
    }

    p {
        line-height: 1.25em;
    }

    .caption {
        font-size: 16px;
        /*font-style: italic;*/
        color: #666;
        text-align: center;
        margin-top: 4px;
        margin-bottom: 10px;
    }

    video {
        display: block;
        margin: auto;
    }

    figure {
        display: block;
        margin: auto;
        margin-top: 10px;
        margin-bottom: 10px;
    }

    #bibtex pre {
        font-size: 13.5px;
        background-color: #eee;
        padding: 16px;
    }

    .blue {
        color: #2c82c9;
        font-weight: bold;
    }

    .orange {
        color: #d35400;
        font-weight: bold;
    }

    .flex-row {
        display: flex;
        flex-flow: row wrap;
        justify-content: space-around;
        padding: 0;
        margin: 0;
        list-style: none;
    }

    .paper-btn {
        position: relative;
        text-align: center;

        display: inline-block;
        margin: 8px;
        padding: 8px 8px;

        border-width: 0;
        outline: none;
        border-radius: 2px;

        background-color: #B6486F;
        color: white !important;
        font-size: 20px;
        width: 130px;
        font-weight: 600;
    }

    .paper-btn-parent {
        display: flex;
        justify-content: center;
        margin: 16px 0px;
    }

    .paper-btn:hover {
        opacity: 0.85;
    }

    .container {
        margin-left: auto;
        margin-right: auto;
        padding-left: 16px;
        padding-right: 16px;
    }

    .venue {
        /*color: #B6486F;*/
        font-size: 30px;

    }

</style>

<!-- End : Google Analytics Code-->
<script type="text/javascript" src="../js/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic'
      rel='stylesheet' type='text/css'>
<head>
    <title>Compositional Text-to-Image Generation with Dense Blob Representations</title>
    <meta property="og:description" content="Compositional Text-to-Image Generation with Dense Blob Representations"/>
    <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:creator" content="@ArashVahdat">
    <meta name="twitter:title" content="Compositional Text-to-Image Generation with Dense Blob Representations">
    <meta name="twitter:description"
          content="We propose <i>BlobGEN</i> that uses dense blob representations as groundding inputs to guide text-to-image generation.">
    <meta name="twitter:image" content="">
</head>


<body>
<div class="container">
    <div class="paper-title">
        <h1>Compositional Text-to-Image Generation with Dense Blob Representations</h1>
    </div>

    <div id="authors">
        <center>
            <div class="author-row">
                <div class="col-3 text-center"><a href="https://weilinie.github.io/">Weili Nie</a></div>
                <div class="col-3 text-center"><a href="https://www.sifeiliu.net/">Sifei Liu</a>
                </div>
                <div class="col-3 text-center"><a href="https://mortezamardani.github.io/mardani/">Morteza Mardani</a>
                </div>
                <div class="col-3 text-center"><a href="http://www.cs.cmu.edu/~ILIM/people/chaoliu1/">Chao Liu</a>
                </div>
                <div class="col-3 text-center"><a href="https://research.nvidia.com/person/ben-eckart">Benjamin
                    Eckart</a></div>
                <div class="col-3 text-center"><a href="http://latentspace.cc/">Arash Vahdat</a></div>
            </div>

            <!--            <center>-->
            <!--                <table align=center width=500px>-->
            <!--                    <tr>-->
            <!--                        <td align=center width=200px>-->
            <!--                            <center>-->
            <!--                                <span style="font-size:30px">NVIDIA</span>-->
            <!--                            </center>-->
            <!--                        </td>-->
            <!--                    </tr>-->
            <!--                </table>-->
            <!--            </center>-->

        </center>
        <br>
        <center><img width="20%" src="./assets/nvidialogo.png" style="margin-top: 20px; margin-bottom: 3px;"></center>
        <div class="affil-row">
            <div class="venue text-center"><b>ICML 2024</b></div>
        </div>
        <br>
        <div style="clear: both">
            <div class="paper-btn-parent">
                <a class="paper-btn" href="https://arxiv.org/pdf/xxxx.xxxxx.pdf">
                    <span class="material-icons"> description </span>
                    Paper
                </a>
                <!--                <a class="paper-btn" href="https://github.com/NVlabs/BlobGEN">-->
                <!--                    <span class="material-icons"> code </span>-->
                <!--                    Code (TBD)-->
                <!--                </a>-->
            </div>
        </div>
    </div>

    <section id="abstract"/>
    <h2>Abstract</h2>
    <hr>
    <div class="flex-row">
        <p>
            Existing text-to-image models struggle to follow complex text prompts, raising the need for extra grounding
            inputs for better controllability. In this work, we propose to decompose a scene into visual primitives -
            denoted as dense blob representations - that contain fine-grained details of the scene while being modular,
            human-interpretable, and easy-to-construct. Based on blob representations, we develop a blob-grounded
            text-to-image diffusion model, termed BlobGEN, for compositional generation. Particularly, we introduce a
            new masked cross-attention module to disentangle the fusion between blob representations and visual
            features. To leverage the compositionality of large language models (LLMs), we introduce a new in-context
            learning approach to generate blob representations from text prompts. Our extensive experiments show that
            BlobGEN achieves superior zero-shot generation quality and better layout-guided controllability on MS-COCO.
            When augmented by LLMs, our method exhibits superior numerical and spatial correctness on compositional
            image generation benchmarks.
        </p>
    </div>
    </section>


    <section id="teaser-image">
        <h2>Blob representations and BlobGEN</h2>
        <hr>
        <div class="flex-row">
            <p>
                We introduce a new type of visual layout, termed <b>dense blob representation</b>, to serve as
                grounding inputs to guide text-to-image generation.
                A blob representation corresponds to visual
                primitives (i.e., objects in a scene) and has two components: 1) the blob parameter, which formulates a
                tilted
                ellipse to specify the object's position, size and orientation; and 2) the blob description, which is a
                rich text sentence that describes the object's appearance, style, and visual attributes.
                The blob representations can be <b>automatically extracted</b> from a scene: 1) We first apply
                an open-vocabulary segmentation model to get instance segmentation maps, followed by an ellipse fitting
                optimization
                to determine blob parameters for each map.
                <!--                , aiming to maximize the Intersection Over Union (IOU) between-->
                <!--                the blob ellipse and segmentation mask. -->
                2) With segmentation maps, we crop out local regions for all
                objects in an image and use a vision-language model to caption each blob.
            </p>
        </div>
        <!--        </p>-->
        <figure style="margin-top: 20px; margin-bottom: 20px;">
            <a href="assets/blobgen_teaser_v2.png">
                <center><img width="80%" src="assets/blobgen_teaser_v1.png"></center>
            </a>
            <p class="caption">
                <!--                We visualize the <b>dense blob representation</b> and how we extract them automatically from a scene.-->
                <!--                <b>Blob representations</b>. -->
                We extract blob representations (parameters and descriptions) using existing tools to guide the
                text-to-image diffusion model.
            <p class="caption">
            </p>
        </figure>

        <div class="flex-row">
            <p>
                We develop a blob-grounded text-to-image diffusion model, termed <b>BlobGEN</b>, that is built upon
                existing diffusion models (such as Stable Diffusion), with blob representations as grounding input.
                To disentangle the fusion between blob representations and visual features,
                we devise a <b>masked cross-attention</b> module that relates each blob to the corresponding visual
                feature
                solely in its local region.
                With this masking design, blob representations and local visual features are well aligned in an explicit
                manner. Therefore, the blob grounding process can be more modular and independent across different
                object regions, and the model can be more disentangled in generation.
                <!--                Furthermore,-->
                <!--                we design a new in-context learning approach for LLMs to generate dense blob representations from text-->
                <!--                prompts. By augmenting our model with LLMs, we can leverage-->
                <!--                the visual understanding and compositional reasoning capabilities-->
                <!--                of LLMs to solve complex compositional image generation tasks.-->
                <!--                Our model paves the way for a modular framework where images can be easily generated or manipulated by-->
                <!--                users and LLMs.-->
            </p>
        </div>
        <figure style="margin-top: 20px; margin-bottom: 20px;">
            <a href="assets/blobgen_teaser_v2.png">
                <center><img width="80%" src="assets/blobgen_teaser_v2.png"></center>
            </a>
            <p class="caption">
                BlobGEN leverages a novel masked cross-attention module that allows
                visual features to attend to only corresponding blobs.
            </p>
            <p class="caption">
            </p>
        </figure>
    </section>


    <section id="results_blobgen_reconstruction">
        <h2>Blob representations capture fine-grained details of real images</h2>
        <hr>
        <div class="flex-row">
            <p>
                We show visual examples of how blob-grounded generation (BlobGEN) can capture fine-grained
                details of real images, in particular those with irregular
                objects, large objects and background. For example, the first three examples in (a) show that blob
                representations can capture "a person with waving hands outside the blob". the last two examples in (a)
                show that blob representations can capture "a river with irregular shapes". Also, the fourth example in
                (b)
                also shows that blob representations can capture "the great wall with a zigzag shape". As for large
                objects and background, the first two rows in (b) show that blob representations can capture "sky" with
                the similar color and mood. The second row in (b) show that blob representations can capture the large
                "pier" with a similar color, pattern and shape. The third row in (b) show that blob representations
                can capture the large
                "foggy grass" and its reflection in the water. The last two rows in (b) show that blob representations
                can capture the large mountains in the background.
            </p>
        </div>
        <figure style="width: 100%;">
            <a href="assets/blobs_irregular.png">
                <center><img width="100%" src="assets/blobs_irregular.png"></center>
            </a>
            <p class="caption" style="margin-bottom: 24px;">
                Examples of blob-grounded generation capturing (a) irregular objects (e.g. "person with waving
                hands" and "river"), and (b) large objects and background (e.g., "sky", "mountain" and "grass").
                From left to right in each row, we show the reference real image, blobs, generated image and
                generated image superposed on blobs. Note that images have been compressed to save memory. Better
                zoom in for better visualization.
            </p>
        </figure>
    </section>


    <section id="results_blob_box_comparison">
        <h2>Comparison between of blob- and box-grounded generation</h2>
        <hr>
        <div class="flex-row">
            <p>
                To further demonstrate that blob representations can capture more fine-grained information than bounding
                boxes, we provide qualitative results of comparing blob-grounded generation (Ours) and box-grounded
                generation (GLIGEN [1]). We can clearly see that generated images from blob representations can better
                reconstruct fine-grained details of the reference image than those from bounding boxes. For instance, in
                the first example, the
                laptop's orientation and the two books' orientation and color in the generated image from our method
                closely follow the
                reference image while GLIGEN does not. In the second example, the car's color and orientation and the
                grass and road's appearance and orientation from our method also more closely follow the
                reference image. In the third example, the two giraffes' orientation is better captured by blob
                representations than bounding boxes, with a direct comparison to the reference. In the last example, the
                two girls' appearance and pose, the couch's color, and the orientation and appearance of the
                counter and chair in the background are also better captured by blob representations.
            </p>
        </div>
        <figure style="width: 100%;">
            <a href="assets/blobs_comp_gligen.png">
                <center><img width="100%" src="assets/blobs_comp_gligen.png"></center>
            </a>
            <p class="caption" style="margin-bottom: 24px;">
                More qualitative results of comparing blob-grounded generation (Ours) and box-grounded generation
                (GLIGEN [1]). In each row, we visualize the reference real image (Left), bounding boxes and GLIGEN
                generated image (Middle), blobs and our generated image (Right). We also highlight the salient objects
                in each example to guide the scrutinization of how the respective generated image differs from reference
                image. Note that images have been compressed to save memory. Better zoom in for better
                visualization.
            </p>
        </figure>
    </section>


    <section id="results_blobgen_edit">
        <h2>Manipulating blobs to unlock various local image editing tasks</h2>
        <hr>
        <div class="flex-row">
            <p>
                We visualize various image editing results by manually changing a blob representation (e.g.,
                either blob description or blob parameter) while keeping other blobs the same. Our
                method can enable different local editing capabilities by solely changing the corresponding blob
                descriptions. In particular, we can change the object color, category, and texture while keeping the
                unedited regions mostly the same. Furthermore, our method can also make different object repositioning
                tasks easily achievable by solely manipulating the corresponding blob parameters. For instance, we can
                move an object to different locations, change an object's orientation, and add/remove an object while
                also keeping other regions nearly unchanged. Note that we have not applied any attention map guidance or
                sample blending trick during sampling, to preserve
                localization and disentanglement in the editing process. Thus, these results demonstrate that a
                well-disentangled and modular property naturally emerges in our blob-grounded generation framework.
            </p>
        </div>
        <figure style="width: 100%;">
            <a href="assets/blobgen_edit_v3.png">
                <center><img width="100%" src="assets/blobgen_edit_v3.png"></center>
            </a>
            <p class="caption" style="margin-bottom: 24px;">
                Various local image editing results of BlobGEN, where each example contains
                two generated images: (Left) original setting and (Right) after editing. The top two rows show the local
                editing results where we only change the blob description and since the blob parameters stay the same
                after editing, we do not show blob visualizations. The bottom four rows show the object reposition
                results where we only change the blob parameter. Note that images have been compressed to save memory.
                Better zoom in for better visualization.
            </p>
        </figure>
    </section>


    <section id="results_llm_for_blobs">
        <h2>Compositional generation from blobs controlled by LLMs</h2>
        <hr>
        <div class="flex-row">
            <p>
                Blob representations can be generated by LLMs from a global caption. To demonstrate the blob
                control from LLMs, we consider four cases of how LLMs understand
                compositional prompts for correct visual planning: (a) swapping object name ("cat" <-> "car"), (b)
                changing relative reposition ("left" <-> "right"), (c) changing object number ("three" <-> "four"), and
                (d) swapping object number ("one bench & two cats" <-> "two benches & one cat"). We can see that LLMs
                have the ability of capturing the subtle differences when the prompts are modified in an "adversarial"
                manner. Besides, LLMs can generate diverse and feasible visual layouts from the same prompt, which
                BlobGEN can use for robustly synthesizing correct images of high quality and diversity.
            </p>
        </div>
        <figure style="width: 100%;">
            <a href="assets/blobs_llm_control.png">
                <center><img width="100%" src="assets/blobs_llm_control.png"></center>
            </a>
            <p class="caption" style="margin-bottom: 24px;">
                Qualitative results of blob control over using LLMs, where we consider four cases of how LLMs understand
                compositional prompts for correct visual planning: (a) swapping object name ("cat" <-> "car"), (b)
                changing relative reposition ("left" <-> "right"), (c) changing object number ("three" <-> "four"), and
                (d) swapping object number ("one bench & two cats" <-> "two benches & one cat"). In each example, we
                show diverse blobs generated by LLMs (bottom) and the corresponding images generated by BlobGEN (top)
                from the
                same text prompt. Note that images have been compressed to save memory. Better zoom in for better
                visualization.
            </p>
        </figure>
    </section>


    <section id="results_blobgen_counting_comp"/>
    <h2>Comparison with previous methods on compositional generation</h2>
    <hr>
    <div class="flex-row">
        <p>
            We show the numerical and spatial reasoning results of our method and previous approaches in
            the following two figures, respectively. In addition, we visualize
            the layouts inferred by GPT4 for both GLIGEN and our method. We can see that all methods without layout
            planning in the middle always fail in the task, including SDXL that has a large model
            size and more advanced training procedures, and Attention-and-Excite that has an
            explicit attention guidance. Compared with LayoutGPT based on GLIGEN, our method can not only generate
            images with better spatial and numerical correctness, but also in general has better visual quality with
            less ``copy-and-paste'' effect.

        </p>
    </div>
    <figure style="width: 100%;">
        <a href="assets/blobgen_counting_comp.png">
            <center><img width="100%" src="assets/blobgen_counting_comp.png"></center>
        </a>
        <p class="caption" style="margin-bottom: 24px;">
            Qualitative results of various methods in <b>numerical reasoning</b> tasks on the NSR-1K benchmark [2]. In
            our method, given a caption,
            we
            prompt GPT4 to generate blob parameters (Left) and LLAMA-13B to generate blob descriptions (not shown in the
            figure), which are passed to BlobGEN to synthesize an image
            (Right). Note that images have been
            compressed to save memory. Better zoom in for better visualization.
        </p>
    </figure>
    <figure style="width: 100%;">
        <a href="assets/blobgen_spatial_comp.png">
            <center><img width="100%" src="assets/blobgen_spatial_comp.png"></center>
        </a>
        <p class="caption" style="margin-bottom: 24px;">
            Qualitative results of various methods in <b>spatial reasoning</b> tasks on the NSR-1K benchmark [2]. In our
            method, given a caption, we
            prompt GPT4 to generate blob parameters (Left) and LLAMA-13B to generate blob descriptions (not shown in the
            figure), which are passed to BlobGEN to synthesize an image
            (Right). Note that images have been
            compressed to save memory. Better zoom in for better visualization.
        </p>
    </figure>
    </section>


    <section id="results1"/>
    <h2>Comparison with LMD [3] using fixed in-context examples for LLMs</h2>
    <hr>
    <div class="flex-row">
        <p>
            We qualitatively compare our method and LMD [3] on the NSR-1K benchmark. For a fair comparison, we use <b>the
            same 8 fixed in-context demo examples</b> (without retrieval) for GPT-4 to generate bounding boxes for LMD
            and blobs for our
            method, respectively. We observe that 1) for some complex examples, such as "a boat to the right of a fork"
            in (a) and "there are one car with three motorcycles in the image" in (b), LMD fails but our method works;
            2)
            LMD consistently has the "copy-and-paste" artifact in its generated images (since it modifies the diffusion
            sampling process for compositionality), such as "a teddy bear to the left of a potted plant" in
            (a) and "a photo of four boats" in (b), while our generation looks much more natural; and 3) the
            sampling time of LMD is around 3x slower than our method.

        </p>
    </div>
    <figure style="width: 100%;">
        <a href="assets/blobs_comp_lmd.png">
            <center><img width="100%" src="assets/blobs_comp_lmd.png"></center>
        </a>
        <p class="caption" style="margin-bottom: 24px;">
            Qualitative results of comparing our method with LMD [3] on the NSR-1K benchmark proposed by LayoutGPT [2]
            for spatial and numerical reasoning. In each example, we first prompt GPT-4 to generate boxes for LMD and
            blobs for our method, respectively, with <b>the same 8 fixed in-context demo examples</b>. The generated
            boxes and blobs are passed to LMD and BlobGEN to generate images, respectively. Note that images have been
            compressed to save memory. Better zoom in for better visualization.
        </p>
    </figure>
    </section>


    <section id="advantages"/>
    <h2>References</h2>
    <hr>
    <div class="flex-row">
        <p>
            [1] Y. Li, et al. "GLIGEN: Open-set grounded text-to-image generation.", CVPR 2023.
            <br>
            [2] W. Feng, et al. "LayoutGPT: compositional visual planning and generation with large language models.",
            NeurIPS 2023.
            <br>
            [3] L. Lian, et al. "LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion
            Models with Large Language Models.", TMLR 2023.
        </p>
    </div>
    </section>


    <!--    <section id="advantages"/>-->
    <!--    <h2>Broader impact and limitations</h2>-->
    <!--    <hr>-->
    <!--    <div class="flex-row">-->
    <!--        <p>-->
    <!--            Our blob-grounded text-to-image model is based on a pre-trained text-to-image model, so it may inherit the-->
    <!--            potential biases and malicious information from the base model. Because our approach improves both the-->
    <!--            generation quality and the user controllability over the base model, on the positive side, it will improve-->
    <!--            the efficiency of human users in using generative models for creative work; on the negative side, similar to-->
    <!--            any generative AI tool, it can be used to generate malicious content. -->
    <!--            Furthermore, when augmented by LLMs for layout planning, our method will simplify the layout designing-->
    <!--            process, resulting in less burden on human designers for content creation. But we also note that the biases-->
    <!--            and misinformation from LLMs could also harm the use of our approach without proper regulation.-->
    <!--        </p>-->
    <!--        <p>-->
    <!--            Our work has several limitations that we leave for the future work. First, even though blob representations-->
    <!--            can preserve fine-grained details of the image, we cannot solely rely on them to perfectly recover the-->
    <!--            original image, where a combination with inversion methods is still needed. Second,-->
    <!--            we see some failure cases for image editing, which we believe-->
    <!--            advanced editing techniques can be applied to alleviate. Third, we also see some-->
    <!--            failure cases in the numerical and spatial reasoning tasks. It-->
    <!--            is an interesting challenge to further improve the integration between LLMs and blob-grounded generation.-->
    <!--        </p>-->
    <!--    </div>-->
    <!--    </section>-->

    <section id="paper">
        <h2>Paper</h2>
        <hr>
        <div class="flex-row">
            <div style="box-sizing: border-box; padding: 16px; margin: auto;">
                <a href="https://arxiv.org/pdf/xxxx.xxxxx.pdf"><img class="screenshot" src="assets/blobgen_preview.png"></a>
            </div>
            <div style="width: 50%; font-size: 20px;">
                <p><b>Compositional Text-to-Image Generation with Dense Blob Representations</b></p>
                <p>Weili Nie, Sifei Liu, Morteza Mardani, Chao Liu, Benjamin Eckart, Arash Vahdat</p>
                <div><span class="material-icons"> description </span><a href="https://arxiv.org/abs/xxxx.xxxxx"> arXiv
                    version</a></div>
                <div><span class="material-icons"> insert_comment </span><a href="assets/nie2024blobgen.bib">
                    BibTeX</a>
                </div>
                <!--                <div><span class="material-icons"> integration_instructions </span><a-->
                <!--                        href="https://github.com/NVlabs/BlobGEN"> Code (TBD)</a></div>-->
                <!--            </div>-->
            </div>
    </section>

    <section id="bibtex">
        <h2>Citation</h2>
        <hr>
        <pre><code>@inproceedings{nie2024BlobGEN,
  title={Compositional Text-to-Image Generation with Dense Blob Representations},
  author={Nie, Weili and Liu, Sifei and Mardani, Morteza and Liu, Chao and Eckart, Benjamin and Vahdat, Arash},
  booktitle = {International Conference on Machine Learning (ICML)},
  year={2024}
}</code></pre>
    </section>


</div>
</body>
</html>